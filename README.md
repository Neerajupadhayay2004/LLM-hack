# LLM-Hack: Intelligent Query-Retrieval System

<img width="1920" height="1080" alt="Screenshot at 2025-08-07 01-35-32" src="https://github.com/user-attachments/assets/ac3a9b67-7c49-43fd-af72-78f69b3e9253" />

<img width="1920" height="1080" alt="Screenshot at 2025-08-07 01-35-40" src="https://github.com/user-attachments/assets/4cbe71a7-52a7-4e36-a675-6d60fbf582a6" />


<img width="1920" height="1080" alt="Screenshot at 2025-08-07 01-35-48" src="https://github.com/user-attachments/assets/e8b58e20-514e-4fdb-9252-aa84ef2324b9" />

![Screenshot at 2025-08-07 01-35-52](https://github.com/user-attachments/assets/7e7ee4e0-0113-4639-908b-826cab4e078a)


[![Deployed on Vercel](https://img.shields.io/badge/Deployed%20on-Vercel-000000?logo=vercel)](https://v0-llm0mmain.vercel.app)
[![Built with v0](https://img.shields.io/badge/Built%20with-v0-FF6154)](https://v0.dev)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.68+-00a393)](https://fastapi.tiangolo.com)
[![Next.js](https://img.shields.io/badge/Next.js-14+-000000?logo=nextdotjs)](https://nextjs.org)

An LLM-powered intelligent query-retrieval system that processes large documents and makes contextual decisions for insurance, legal, HR, and compliance domains.

## ğŸŒ Live Demo

- **Frontend:** (https://v0-llm0mmain.vercel.app)

## ğŸ¯ Problem Statement

Design an intelligent system that can:
- Process PDFs, DOCX, and email documents
- Handle policy/contract data efficiently
- Parse natural language queries
- Provide explainable decision rationale with structured JSON responses

### Sample Query
> "Does this policy cover knee surgery, and what are the conditions?"

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input Docs     â”‚â”€â”€â”€â–¶â”‚  LLM Parser     â”‚â”€â”€â”€â–¶â”‚ Embedding Searchâ”‚
â”‚  PDF/DOCX/Email â”‚    â”‚ Extract Query   â”‚    â”‚ FAISS/Pinecone  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â–¼                        â–²
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  JSON Output    â”‚â—€â”€â”€â”€â”‚ Logic Evaluationâ”‚â—€â”€â”€â”€â”‚ Clause Matching â”‚
â”‚ Structured Resp â”‚    â”‚ Decision Processâ”‚    â”‚ Semantic Search â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Tech Stack

### Frontend
- **Next.js 14+** - React framework
- **TypeScript** - Type-safe development
- **Tailwind CSS** - Utility-first styling
- **shadcn/ui** - Component library

### Backend
- **FastAPI** - High-performance Python web framework
- **PostgreSQL** - Primary database
- **Docker** - Containerization

### AI/ML Components
- **GPT-4** - Large Language Model
- **Pinecone** - Vector database for semantic search
- **LangChain** - LLM application framework
- **OpenAI Embeddings** - Text vectorization

## ğŸ“ Project Structure

```
LLM-hack/
â”œâ”€â”€ analytics/          # Analytics and monitoring
â”œâ”€â”€ api/               # FastAPI backend
â”œâ”€â”€ app/               # Next.js application
â”œâ”€â”€ components/        # React components
â”œâ”€â”€ config/           # Configuration files
â”œâ”€â”€ lib/              # Utility libraries
â”œâ”€â”€ models/           # Data models
â”œâ”€â”€ public/           # Static assets
â”œâ”€â”€ services/         # Service layer
â”œâ”€â”€ styles/           # CSS styles
â”œâ”€â”€ ui/               # UI components
â”œâ”€â”€ main.py           # FastAPI entry point
â”œâ”€â”€ requirements.txt  # Python dependencies
â”œâ”€â”€ package.json      # Node.js dependencies
â”œâ”€â”€ docker-compose.yml # Docker configuration
â””â”€â”€ README.md         # This file
```

## ğŸ› ï¸ Installation & Setup

### Prerequisites
- Node.js 18+
- Python 3.9+
- Docker & Docker Compose
- Pinecone Account
- OpenAI API Key

### Quick Start

1. **Clone the repository**
   ```bash
   git clone https://github.com/Neerajupadhayay2004/LLM-hack.git
   cd LLM-hack
   ```

2. **Environment Setup**
   ```bash
   # Copy environment file
   cp .env.example .env
   
   # Add your API keys
   PINECONE_API_KEY=your_pinecone_key
   OPENAI_API_KEY=your_openai_key
   DATABASE_URL=postgresql://user:pass@localhost:5432/llmhack
   ```

3. **Docker Deployment (Recommended)**
   ```bash
   docker-compose up -d
   ```

4. **Manual Setup**
   
   **Backend:**
   ```bash
   cd api
   pip install -r requirements.txt
   python main.py
   ```
   
   **Frontend:**
   ```bash
   npm install
   npm run dev
   ```

5. **Database Setup**
   ```bash
   python setup_database.py
   ```

## ğŸ“¡ API Documentation

### Base URL
- **Local:** `http://localhost:8000/api/v1`
- **Production:** `https://your-api-domain.com/api/v1`

### Authentication
```
Authorization: Bearer a4f025be07025e89076181fecc043bf8b5222b260bff689775053561aa3175e6
```

### Endpoints

#### Process Document Query
```http
POST /hackrx/run
```

**Request:**
```json
{
    "documents": "https://hackrx.blob.core.windows.net/assets/policy.pdf",
    "questions": [
        "What is the grace period for premium payment?",
        "Does this policy cover maternity expenses?"
    ]
}
```

**Response:**
```json
{
    "answers": [
        "A grace period of thirty days is provided for premium payment...",
        "Yes, the policy covers maternity expenses with 24-month waiting period..."
    ]
}
```

#### Upload Document
```http
POST /upload
```

#### Health Check
```http
GET /health
```

## ğŸ§ª Testing

### Run API Tests
```bash
python test_api.py
```

### Sample cURL Request
```bash
curl -X POST "http://localhost:8000/api/v1/hackrx/run" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer a4f025be07025e89076181fecc043bf8b5222b260bff689775053561aa3175e6" \
  -d '{
    "documents": "policy.pdf",
    "questions": ["What is covered under this policy?"]
  }'
```

## ğŸ“Š Evaluation Parameters

Our solution is optimized for:

- **ğŸ¯ Accuracy** - Precision in query understanding and clause matching
- **âš¡ Token Efficiency** - Optimized LLM usage and cost-effectiveness
- **ğŸš€ Latency** - Real-time response performance
- **ğŸ”„ Reusability** - Modular and extensible codebase
- **ğŸ“ Explainability** - Clear decision reasoning with clause traceability

## ğŸ¤ Team Structure

This project is designed for a 4-member team:

1. **Backend & API Developer** - FastAPI, PostgreSQL, Docker
2. **Document Processing Engineer** - PDF/DOCX parsing, text chunking
3. **Vector Search & AI Specialist** - Pinecone, embeddings, semantic search
4. **LLM & Response Generator** - GPT-4 integration, prompt engineering

## ğŸš€ Deployment

### Vercel (Frontend)
The frontend is automatically deployed to Vercel and synced with v0.dev changes.

### Docker (Full Stack)
```bash
docker-compose up --build -d
```

### Manual Production Setup
1. Set up PostgreSQL database
2. Configure Pinecone index
3. Deploy FastAPI backend
4. Build and deploy Next.js frontend

## ğŸ”§ Development

### Start Development Server
```bash
# Backend
python main.py

# Frontend
npm run dev
```

### Environment Variables
```env
# Database
DATABASE_URL=postgresql://user:pass@localhost:5432/llmhack

# AI Services
OPENAI_API_KEY=your_openai_key
PINECONE_API_KEY=your_pinecone_key
PINECONE_ENV=us-west1-gcp

# App Configuration
BEARER_TOKEN=a4f025be07025e89076181fecc043bf8b5222b260bff689775053561aa3175e6
```

## ğŸ“ˆ Performance Optimization

- **Caching:** Redis for frequent queries
- **Chunking:** Optimized text splitting (1000 chars)
- **Embeddings:** Batch processing for efficiency
- **Response:** Streaming for large documents

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [v0.dev](https://v0.dev) for the rapid UI development
- [LangChain](https://langchain.com) for LLM orchestration
- [Pinecone](https://pinecone.io) for vector database
- [OpenAI](https://openai.com) for GPT-4 and embeddings

## ğŸ“ Contact

- **GitHub:** [Neerajupadhayay2004](https://github.com/Neerajupadhayay2004)
- **Project Link:** [https://github.com/Neerajupadhayay2004/LLM-hack](https://github.com/Neerajupadhayay2004/LLM-hack)

---

**Built with â¤ï¸ for intelligent document processing**
